{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Explaining Logistic Regression`\n",
    "\n",
    "# <font color=red>Mr Fugu Data Science</font>\n",
    "\n",
    "# (◕‿◕✿)\n",
    "\n",
    "# Purpose & Outcome:\n",
    "\n",
    "+ Cover the theory of Logistic Regression and get a feel of the underworkings\n",
    "+ Some plotting and examples to help \n",
    "\n",
    "# `Let me know if there are any videos you would like to see.`\n",
    "\n",
    "\n",
    "# <font color=red>Check me out on Buy Me A Coffee:</font> `@mrfugudatasci`\n",
    "\n",
    "`--------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `Logistic (Logit) regression` is used for `Classification`. We showed an example of this in the previous notebook (*`skLearn_01`*) where we looked for Ham/Spam for sms messages. \n",
    "\n",
    "+ You are trying to estimate the probability that some instance, belongs to a particular class. \n",
    "    + Using logistic regression, you are preserving the marginal probabilities of training data.\n",
    "    + You are also using the coefficients to aide in detecting what input variables are important. \n",
    "\n",
    "`--------------------------------------`\n",
    "\n",
    "**But, why not use `Linear Regression`?**\n",
    "\n",
    "+ In order to use `Linear Regression` we would have to create a threshold for the classification. \n",
    "    + This is a problem if we are only trying to do a (Pass/Fail).\n",
    "    + Unbounded results; unlike when using logistic which are [0:1]\n",
    "        + Hmm, what does that mean?\n",
    "            * Well: Linear regression will have an instance where the values are > 1 or <0. \n",
    "    + Outliers, are a problem because they will change the best fit line and increase error.\n",
    "\n",
    "\n",
    "# `Differences:`\n",
    "\n",
    "+ `Linear Regression:`\n",
    "    * Fits a line, to the points (data) which can be used to estimate/predicate a new value\n",
    "    \n",
    "\n",
    "+ `Logistic Regression:`\n",
    "    * fits a line to best separate between classes\n",
    "\n",
    "`--------------------------------------`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `What are we trying to find?`\n",
    "\n",
    "+ The parameters \"coefficients\" <font size=5>$\\beta$</font> are estimated using one of two methods:\n",
    "\n",
    "    * `Least Squares Optimization:` using iterative approach [IRWLS](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares) \n",
    "    * `Max Likelihood Estimation` \n",
    "    \n",
    "\n",
    "    \n",
    "# `What Are We Exactly Calculating with Logistic Regression?`\n",
    "\n",
    "+ `You are computing a weighted sum of input features, plus a bias 'intercept' term`\n",
    "    + There is one coefficient per input.\n",
    "    \n",
    "`Starting from Linear Regression`: we have our hypothesis <font size=4>f ($x_i$)</font> representing the predicted response for the <font size=4>$i^{th}$</font> oberservation of <font size=4>$x_i$</font>.\n",
    "\n",
    "<font size=5>$P(X) =f(x_i) = \\hat{Y} = \\beta_0 + \\beta_1x_{i1}+ \\beta_2x_{i2}+...+\\beta_m x_{im}$</font>\n",
    "\n",
    "At this point the output would be a (real) value, not a class label. At this point we need to do some work and make this output what we want [0,1]\n",
    "\n",
    "`-----------------------------`\n",
    "\n",
    "# Here comes the `Logistic Function`\n",
    "\n",
    "`Logistic Function:` <font size=5>$f(x) = \\frac{1}{(1 + e^{-x})}$</font>\n",
    "\n",
    "For input <font size=4>( x )</font> we will replace with the weighted sum\n",
    "\n",
    "\n",
    "<font size=5>$f(x) = \\frac{1}{(1 + e^{- (\\beta_o +\\beta x)})}$</font>\n",
    "\n",
    "# Now, we have to get the `Log-Odds:`\n",
    "\n",
    "+ Think of when you did an introduction to probability and were introduced to gambling: (wins:loses) ratio.\n",
    "\n",
    "We can convert that to a probability of success: <font size=5>$\\frac{P}{1-P}$</font>\n",
    "\n",
    "`And to get the log-odds for success:` <font size=5>$log(\\frac{P}{1-P})$</font>\n",
    "\n",
    "Now we will plug this into our logistic function from above:\n",
    "\n",
    "<font size=5>$log(\\frac{P}{1-P}) = \\beta_o +\\beta X$</font>\n",
    "\n",
    "<font size=5>$\\frac{P}{1-P} = e^{\\beta_o +\\beta X}$</font>\n",
    "\n",
    "<font size=5>$P = e^{\\beta_o +\\beta X}(1-P)$</font>\n",
    "\n",
    "<font size=5>$P = e^{\\beta_o +\\beta X}-P(e^{\\beta_o +\\beta X}))$</font>\n",
    "\n",
    "<font size=5>$P + P(e^{\\beta_o +\\beta X})= e^{\\beta_o +\\beta X}$</font>\n",
    "\n",
    "<font size=5>$P + (1 + e^{\\beta_o +\\beta X})=e^{\\beta_o +\\beta X}$</font>\n",
    "\n",
    "<font size=5>$P = \\frac{e^{\\beta_o +\\beta X}}{(1 + e^{\\beta_o +\\beta X})}$</font>\n",
    "\n",
    "<font size=5>$P = \\frac{e^{\\beta_o +\\beta X}}{\\frac{e^{\\beta_o +\\beta X}}{e^{\\beta_o +\\beta X}}+ e^{\\beta_o +\\beta X}}$</font>\n",
    "\n",
    "<font size=5>$P = \\frac{e^{\\beta_o +\\beta X}}{e^{\\beta_o +\\beta X}[{\\frac{1}{e^{\\beta_o +\\beta X}}+1}]}$</font>\n",
    "\n",
    "<font size=5>$P = \\frac{1}{(1 + e^{- \\beta_o +\\beta X})}$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Bernoulli`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`--------------`\n",
    "\n",
    "+ **`Decission Boundary Consideration:`** Ideally, you aim for `Recall & Precision =1`\n",
    "    * `Low Recall: High Precision:` situation where you may want to `reduce number of false positives without adjusting false negaitives` you will choose a value with either a `High Recall or Low Precision`.\n",
    "        + This can occur in situations where you have adevertising and you want to make a clear, positive impression.  \n",
    "    * `Low Precision: High Recall:` opposite of above.\n",
    "        + Think of falsly labeling someone with cancer as not having cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Log Likelihood:`\n",
    "\n",
    "+ Since, `logistic regression` predicts probabilities and not just classes we can use the likelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations & Help\n",
    "\n",
    "# ◔̯◔\n",
    "\n",
    "https://medium.com/@lily_su\n",
    "\n",
    "https://medium.com/@lily_su/log-linear-regression-85ed7f1a8f24\n",
    "\n",
    "https://realpython.com/logistic-regression-python/ (very good code examples)\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html\n",
    "\n",
    "https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ch04.html\n",
    "\n",
    "https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n",
    "\n",
    "https://www.pluralsight.com/guides/linear-lasso-ridge-regression-scikit-learn\n",
    "\n",
    "https://www.kdnuggets.com/2019/05/modeling-price-regularized-linear-model-xgboost.html\n",
    "\n",
    "https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\n",
    "\n",
    "https://github.com/SSaishruthi/LogisticRegression_Vectorized_Implementation/blob/master/Logistic_Regression.ipynb\n",
    "\n",
    "https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/ (good overall concept explanation)\n",
    "\n",
    "\n",
    "\n",
    "`Math`\n",
    "\n",
    "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/pdfs/40%20LogisticRegression.pdf\n",
    "\n",
    "https://win-vector.com/2011/09/14/the-simpler-derivation-of-logistic-regression/\n",
    "\n",
    "https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d\n",
    "\n",
    "https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf\n",
    "\n",
    "http://personal.psu.edu/jol2/course/stat597e/notes2/logit.pdf\n",
    "\n",
    "https://www.kaggle.com/hamzafar/derivation-in-context-of-logistic-regression\n",
    "\n",
    "http://www.utstat.toronto.edu/~brunner/oldclass/appliedf11/handouts/2101f11LogisticRegression.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
